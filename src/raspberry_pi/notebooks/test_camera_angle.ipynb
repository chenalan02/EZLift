{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from picamera2 import Picamera2\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "PALLET_CLS_ID = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path = \"/home/pi/EZLift/src/raspberry_pi/custom_palletsonly_50epochs_edgetpu.tflite\"\n",
    "\n",
    "model = YOLO(model_path, task='detect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hough_line_detection(img, canny_low=150, canny_high=450, hough_thresh=45, min_len=20, max_gap=7):\n",
    "    # Get edges using Canny\n",
    "    edges = edges = cv2.Canny(img, canny_low, canny_high)\n",
    "\n",
    "    # Detect lines using Hough Transform\n",
    "    lines = cv2.HoughLinesP(edges, \n",
    "                            rho=1,            # Distance resolution of accumulator in pixels\n",
    "                            theta=np.pi / 180, # Angle resolution of accumulator in radians\n",
    "                            threshold=hough_thresh, # Minimum number of votes (intersections in Hough grid cell)\n",
    "                            minLineLength=min_len,  # Minimum length of line (pixels)\n",
    "                            maxLineGap=max_gap)     # Maximum allowed gap between points on the same line\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_most_conf_box(bbox, conf, cls_id):\n",
    "    best_i = 0\n",
    "    best_conf = 0\n",
    "    for i in range(len(conf)):\n",
    "        if cls_id[i] == PALLET_CLS_ID:\n",
    "            if conf[i] > best_conf:\n",
    "                best_conf = conf[i]\n",
    "                best_i = i\n",
    "    return bbox[best_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def _get_angle_from_lines(lines, angle_thresh=2):\n",
    "\n",
    "    lines_polar = []\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            dx = x2 - x1\n",
    "            dy = y2 - y1\n",
    "            angle_rad = math.atan2(-dy, dx)\n",
    "            angle_deg = math.degrees(angle_rad)\n",
    "            angle_deg = (angle_deg + 360) % 360\n",
    "            length = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "            lines_polar.append((angle_deg, length))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    lines_polar = sorted(lines_polar, key=lambda x: x[0])\n",
    "    groups = []\n",
    "    current_group = [lines_polar[0]]\n",
    "\n",
    "    for angle, length in lines_polar[1:]:\n",
    "        if abs(angle - current_group[-1][0]) < angle_thresh:\n",
    "            current_group.append((angle, length))\n",
    "        else:\n",
    "            groups.append(current_group)\n",
    "            current_group = [(angle, length)]\n",
    "    groups.append(current_group)\n",
    "\n",
    "    best_metric = 0\n",
    "    best_angle = None\n",
    "    for group in groups:\n",
    "        avg_length = np.mean([length for _, length in group])\n",
    "        if len(group) * avg_length**1.5 > best_metric:\n",
    "            avg_angle = np.mean([angle for angle, _ in group])\n",
    "            if not (avg_angle < 90 + angle_thresh and avg_angle > 90 - angle_thresh):\n",
    "                best_metric = len(group) * avg_length**1.5\n",
    "                best_angle = np.mean([angle for angle, _ in group])\n",
    "\n",
    "    return best_angle\n",
    "\n",
    "def _get_angle_error(bbox, img):\n",
    "\n",
    "    img_cropped = img[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]\n",
    "    lines = _hough_line_detection(img_cropped)\n",
    "    angle = _get_angle_from_lines(lines)\n",
    "    if angle > 180:\n",
    "        angle -= 180\n",
    "    if angle > 90:\n",
    "        error = angle - 180\n",
    "    else:\n",
    "        error = angle\n",
    "    return error, lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1:14:14.535922523] [3775] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera_manager.cpp:327 \u001b[0mlibcamera v0.4.0+53-29156679\n",
      "[1:14:14.577695339] [3793] \u001b[1;33m WARN \u001b[1;37mRPiSdn \u001b[1;34msdn.cpp:40 \u001b[0mUsing legacy SDN tuning - please consider moving SDN inside rpi.denoise\n",
      "[1:14:14.580329510] [3793] \u001b[1;33m WARN \u001b[1;37mRPI \u001b[1;34mvc4.cpp:393 \u001b[0mMismatch between Unicam and CamHelper for embedded data usage!\n",
      "[1:14:14.581032224] [3793] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mvc4.cpp:447 \u001b[0mRegistered camera /base/soc/i2c0mux/i2c@1/imx219@10 to Unicam device /dev/media0 and ISP device /dev/media2\n",
      "[1:14:14.581096501] [3793] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mpipeline_base.cpp:1121 \u001b[0mUsing configuration file '/usr/share/libcamera/pipeline/rpi/vc4/rpi_apps.yaml'\n",
      "[1:14:14.597353781] [3775] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera.cpp:1202 \u001b[0mconfiguring streams: (0) 3264x2464-RGB888 (1) 3280x2464-SBGGR10_CSI2P\n",
      "[1:14:14.600090636] [3793] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mvc4.cpp:622 \u001b[0mSensor: /base/soc/i2c0mux/i2c@1/imx219@10 - Selected sensor format: 3280x2464-SBGGR10_1X10 - Selected unicam format: 3280x2464-pBAA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /home/pi/EZLift/src/raspberry_pi/custom_palletsonly_50epochs_edgetpu.tflite on device 0 for TensorFlow Lite Edge TPU inference...\n",
      "\n",
      "0: 256x256 (no detections), 29.1ms\n",
      "Speed: 7.7ms preprocess, 29.1ms inference, 2.5ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 20.7ms\n",
      "Speed: 5.5ms preprocess, 20.7ms inference, 1.6ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 18.8ms\n",
      "Speed: 1.8ms preprocess, 18.8ms inference, 1.4ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 17.3ms\n",
      "Speed: 2.8ms preprocess, 17.3ms inference, 1.6ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 17.7ms\n",
      "Speed: 2.7ms preprocess, 17.7ms inference, 1.7ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.7ms\n",
      "Speed: 2.0ms preprocess, 16.7ms inference, 1.7ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.6ms\n",
      "Speed: 2.7ms preprocess, 16.6ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.6ms\n",
      "Speed: 2.2ms preprocess, 16.6ms inference, 1.8ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.8ms\n",
      "Speed: 2.6ms preprocess, 16.8ms inference, 1.5ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 17.3ms\n",
      "Speed: 2.0ms preprocess, 17.3ms inference, 1.5ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 17.1ms\n",
      "Speed: 1.9ms preprocess, 17.1ms inference, 1.5ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.6ms\n",
      "Speed: 2.0ms preprocess, 16.6ms inference, 1.6ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.5ms\n",
      "Speed: 2.0ms preprocess, 16.5ms inference, 1.5ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.8ms\n",
      "Speed: 2.6ms preprocess, 16.8ms inference, 1.4ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.6ms\n",
      "Speed: 2.0ms preprocess, 16.6ms inference, 1.7ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.6ms\n",
      "Speed: 1.8ms preprocess, 16.6ms inference, 1.7ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.3ms\n",
      "Speed: 2.6ms preprocess, 16.3ms inference, 1.7ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.1ms\n",
      "Speed: 2.9ms preprocess, 16.1ms inference, 1.7ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.5ms\n",
      "Speed: 2.6ms preprocess, 16.5ms inference, 1.7ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.3ms\n",
      "Speed: 2.5ms preprocess, 16.3ms inference, 1.6ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.4ms\n",
      "Speed: 3.4ms preprocess, 16.4ms inference, 1.5ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 17.5ms\n",
      "Speed: 2.9ms preprocess, 17.5ms inference, 1.5ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.6ms\n",
      "Speed: 2.0ms preprocess, 16.6ms inference, 1.7ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.4ms\n",
      "Speed: 2.5ms preprocess, 16.4ms inference, 1.5ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.6ms\n",
      "Speed: 2.0ms preprocess, 16.6ms inference, 1.8ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.4ms\n",
      "Speed: 2.7ms preprocess, 16.4ms inference, 1.8ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.5ms\n",
      "Speed: 3.4ms preprocess, 16.5ms inference, 1.5ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.3ms\n",
      "Speed: 4.8ms preprocess, 16.3ms inference, 1.5ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.4ms\n",
      "Speed: 2.6ms preprocess, 16.4ms inference, 1.5ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.2ms\n",
      "Speed: 2.5ms preprocess, 16.2ms inference, 1.6ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.1ms\n",
      "Speed: 2.6ms preprocess, 16.1ms inference, 1.4ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.1ms\n",
      "Speed: 2.9ms preprocess, 16.1ms inference, 1.4ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.0ms\n",
      "Speed: 2.5ms preprocess, 16.0ms inference, 1.4ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.8ms\n",
      "Speed: 2.1ms preprocess, 16.8ms inference, 1.6ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.3ms\n",
      "Speed: 2.6ms preprocess, 16.3ms inference, 1.6ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.1ms\n",
      "Speed: 2.6ms preprocess, 16.1ms inference, 1.4ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.3ms\n",
      "Speed: 2.5ms preprocess, 16.3ms inference, 1.4ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.4ms\n",
      "Speed: 2.0ms preprocess, 16.4ms inference, 1.3ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.2ms\n",
      "Speed: 3.6ms preprocess, 16.2ms inference, 1.3ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.3ms\n",
      "Speed: 1.9ms preprocess, 16.3ms inference, 1.4ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.1ms\n",
      "Speed: 2.7ms preprocess, 16.1ms inference, 1.4ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.0ms\n",
      "Speed: 2.8ms preprocess, 16.0ms inference, 1.6ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.4ms\n",
      "Speed: 2.8ms preprocess, 16.4ms inference, 1.4ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.8ms\n",
      "Speed: 2.9ms preprocess, 16.8ms inference, 1.7ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.4ms\n",
      "Speed: 1.8ms preprocess, 16.4ms inference, 1.6ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.5ms\n",
      "Speed: 1.9ms preprocess, 16.5ms inference, 1.4ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 17.6ms\n",
      "Speed: 2.4ms preprocess, 17.6ms inference, 1.7ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.4ms\n",
      "Speed: 2.6ms preprocess, 16.4ms inference, 1.6ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.2ms\n",
      "Speed: 2.6ms preprocess, 16.2ms inference, 1.5ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.3ms\n",
      "Speed: 2.6ms preprocess, 16.3ms inference, 1.5ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.4ms\n",
      "Speed: 3.4ms preprocess, 16.4ms inference, 1.5ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.6ms\n",
      "Speed: 2.6ms preprocess, 16.6ms inference, 1.4ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.3ms\n",
      "Speed: 2.5ms preprocess, 16.3ms inference, 1.4ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.4ms\n",
      "Speed: 2.0ms preprocess, 16.4ms inference, 1.4ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.3ms\n",
      "Speed: 2.8ms preprocess, 16.3ms inference, 1.4ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.3ms\n",
      "Speed: 2.0ms preprocess, 16.3ms inference, 1.4ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 (no detections), 16.1ms\n",
      "Speed: 2.6ms preprocess, 16.1ms inference, 1.4ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 1 pallet, 16.2ms\n",
      "Speed: 2.8ms preprocess, 16.2ms inference, 3.2ms postprocess per image at shape (1, 3, 256, 256)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bboxes) > \u001b[32m0\u001b[39m:\n\u001b[32m     26\u001b[39m     bbox = _get_most_conf_box(bbox=bboxes, conf=conf, cls_id=cls_id)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     angle, lines = _get_angle_error(bbox, frame)\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(angle)\n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# Visualize the results on the frame\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import cv2\n",
    "\n",
    "picam2 = Picamera2()\n",
    "picam2.preview_configuration.main.size = (3280, 2464)\n",
    "picam2.preview_configuration.main.format = \"RGB888\"\n",
    "picam2.preview_configuration.align()\n",
    "picam2.configure(\"preview\")\n",
    "picam2.start()\n",
    "picam2.start(show_preview=False)\n",
    "\n",
    "while True:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Capture frame-by-frame\n",
    "    frame = picam2.capture_array()\n",
    "\n",
    "    # Run YOLO inference on the frame with imgsz=192\n",
    "    # results = model.predict(frame, device=\"tpu:0\", imgsz=192)\n",
    "    frame = cv2.resize(frame, (256, 256))\n",
    "    results = model.predict(frame, device=\"tpu:0\", imgsz=256)[0]\n",
    "    bboxes = results.boxes.xyxy.cpu().numpy()\n",
    "    conf = results.boxes.conf.cpu().numpy()\n",
    "    cls_id = results.boxes.cls.cpu().numpy()\n",
    "    if len(bboxes) > 0:\n",
    "        bbox = _get_most_conf_box(bbox=bboxes, conf=conf, cls_id=cls_id)\n",
    "        angle, lines = _get_angle_error(bbox, frame)\n",
    "        print(angle)\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        if lines is not None:\n",
    "            for line in lines:\n",
    "                if line is not None:\n",
    "                    x1, y1, x2, y2 = line[0]\n",
    "                    x1 += bbox[0]\n",
    "                    x2 += min(bbox[0], 255)\n",
    "                    y1 += bbox[1]\n",
    "                    y2 += min(bbox[1], 255)\n",
    "                    cv2.line(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green lines with thickness=2\n",
    "    \n",
    "        # Overlay the FPS on the annotated frame\n",
    "        cv2.putText(frame, f\"Angle: {angle:.2f}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow(\"Camera\", frame)\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
